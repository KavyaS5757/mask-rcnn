{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-12T17:50:35.140625Z","iopub.execute_input":"2023-08-12T17:50:35.141400Z","iopub.status.idle":"2023-08-12T17:50:35.153804Z","shell.execute_reply.started":"2023-08-12T17:50:35.141363Z","shell.execute_reply":"2023-08-12T17:50:35.152792Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install transformers onnx onnxruntime","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:50:35.155534Z","iopub.execute_input":"2023-08-12T17:50:35.156153Z","iopub.status.idle":"2023-08-12T17:50:51.574331Z","shell.execute_reply.started":"2023-08-12T17:50:35.156119Z","shell.execute_reply":"2023-08-12T17:50:51.573063Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (1.14.0)\nCollecting onnxruntime\n  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from onnx) (3.20.3)\nRequirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.10/site-packages (from onnx) (4.6.3)\nCollecting coloredlogs (from onnxruntime)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (23.5.26)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.12)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\nInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.15.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Pix2StructForConditionalGeneration\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-docvqa-base\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:50:51.576695Z","iopub.execute_input":"2023-08-12T17:50:51.577005Z","iopub.status.idle":"2023-08-12T17:51:15.570542Z","shell.execute_reply.started":"2023-08-12T17:50:51.576976Z","shell.execute_reply":"2023-08-12T17:51:15.569453Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/4.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab4b50f364b24ff49c622485a5fafe87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8acbd13dc3a4e73bdb54512af83ff91"}},"metadata":{}}]},{"cell_type":"code","source":"input_text = \"Translate this English text to French.\"","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:15.572213Z","iopub.execute_input":"2023-08-12T17:51:15.572871Z","iopub.status.idle":"2023-08-12T17:51:15.578558Z","shell.execute_reply.started":"2023-08-12T17:51:15.572833Z","shell.execute_reply":"2023-08-12T17:51:15.577650Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import Pix2StructForConditionalGeneration\n\nmodel_name = \"google/pix2struct-docvqa-base\"\nmodel = Pix2StructForConditionalGeneration.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:15.583391Z","iopub.execute_input":"2023-08-12T17:51:15.584109Z","iopub.status.idle":"2023-08-12T17:51:20.411188Z","shell.execute_reply.started":"2023-08-12T17:51:15.584055Z","shell.execute_reply":"2023-08-12T17:51:20.410202Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING=1 ","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:20.412794Z","iopub.execute_input":"2023-08-12T17:51:20.413210Z","iopub.status.idle":"2023-08-12T17:51:20.421445Z","shell.execute_reply.started":"2023-08-12T17:51:20.413174Z","shell.execute_reply":"2023-08-12T17:51:20.416720Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:20.422631Z","iopub.execute_input":"2023-08-12T17:51:20.423256Z","iopub.status.idle":"2023-08-12T17:51:24.805880Z","shell.execute_reply.started":"2023-08-12T17:51:20.423219Z","shell.execute_reply":"2023-08-12T17:51:24.804766Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\nimage_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\nimage = Image.open(requests.get(image_url, stream=True).raw)\n\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-ai2d-base\").to(\"cuda\")\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-ai2d-base\")\n\nquestion = \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"\n\ninputs = processor(images=image, text=question, return_tensors=\"pt\").to(\"cuda\")\n\npredictions = model.generate(**inputs)\nprint(processor.decode(predictions[0], skip_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:24.807235Z","iopub.execute_input":"2023-08-12T17:51:24.808274Z","iopub.status.idle":"2023-08-12T17:51:41.641049Z","shell.execute_reply.started":"2023-08-12T17:51:24.808229Z","shell.execute_reply":"2023-08-12T17:51:41.639928Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/4.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b1b5f1fdce49a2a0eb08370551ca18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/565M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8dd1949a0544649b5376d34a34a4efe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/249 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f4c2b4fd27e4acca9160018946d8392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acf329efc53a4695a04d823d08638b9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/851k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"685492b698874cf795a2c61d4932bb90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/3.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f55cddcf39142be98d77e534a16b74b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6bc4e17e5f4ac08aab858f3e85e5c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/Arial.TTF:   0%|          | 0.00/276k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7dc671af67246c3b2d6b3d6862abf28"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"ash cloud\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = processor(images=image, text=question, return_tensors=\"pt\").to(\"cuda\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:41.642914Z","iopub.execute_input":"2023-08-12T17:51:41.643360Z","iopub.status.idle":"2023-08-12T17:51:41.856848Z","shell.execute_reply.started":"2023-08-12T17:51:41.643301Z","shell.execute_reply":"2023-08-12T17:51:41.855806Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(model)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:41.858391Z","iopub.execute_input":"2023-08-12T17:51:41.858857Z","iopub.status.idle":"2023-08-12T17:51:41.870105Z","shell.execute_reply.started":"2023-08-12T17:51:41.858820Z","shell.execute_reply":"2023-08-12T17:51:41.869083Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Pix2StructForConditionalGeneration(\n  (encoder): Pix2StructVisionModel(\n    (embeddings): Pix2StructVisionEmbeddings(\n      (patch_projection): Linear(in_features=768, out_features=768, bias=True)\n      (row_embedder): Embedding(4096, 768)\n      (column_embedder): Embedding(4096, 768)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): Pix2StructVisionEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x Pix2StructVisionLayer(\n          (attention): Pix2StructVisionAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (mlp): Pix2StructVisionMlp(\n            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=768, bias=False)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (act): NewGELUActivation()\n          )\n          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n          (pre_attention_layer_norm): Pix2StructLayerNorm()\n        )\n      )\n    )\n    (layernorm): Pix2StructLayerNorm()\n  )\n  (decoder): Pix2StructTextModel(\n    (embed_tokens): Embedding(50244, 768)\n    (layer): ModuleList(\n      (0): Pix2StructTextBlock(\n        (self_attention): Pix2StructTextLayerSelfAttention(\n          (attention): Pix2StructTextAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n            (relative_attention_bias): Embedding(32, 12)\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n          (attention): Pix2StructTextAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (mlp): Pix2StructTextLayerFF(\n          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): NewGELUActivation()\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1-11): 11 x Pix2StructTextBlock(\n        (self_attention): Pix2StructTextLayerSelfAttention(\n          (attention): Pix2StructTextAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n          (attention): Pix2StructTextAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (mlp): Pix2StructTextLayerFF(\n          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): NewGELUActivation()\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (final_layer_norm): Pix2StructLayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n    (lm_head): Linear(in_features=768, out_features=50244, bias=False)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nfrom PIL import Image\n\nimage_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\nimage = Image.open(requests.get(image_url, stream=True).raw)\n\ntext_input = \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:41.875483Z","iopub.execute_input":"2023-08-12T17:51:41.876463Z","iopub.status.idle":"2023-08-12T17:51:42.028475Z","shell.execute_reply.started":"2023-08-12T17:51:41.876427Z","shell.execute_reply":"2023-08-12T17:51:42.027352Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import Pix2StructProcessor\nfrom PIL import Image\nimport numpy as np\nimport torch\n\n# Load the Pix2Struct processor\nprocessor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-ai2d-base\")\n\n# Define the image preprocessing function\ndef preprocess_image(image):\n    # Resize the image to the expected input size\n    resized_image = image.resize((2304, 766))  # Adjust the size as needed\n    \n    # Convert the image to a numpy array and normalize pixel values\n    normalized_image = np.array(resized_image) / 255.0\n    \n    # Convert the numpy array to a PyTorch tensor\n    image_tensor = torch.tensor(normalized_image).permute(2, 0, 1).unsqueeze(0).float()\n    \n    return image_tensor\n\n# Load the image\nimage_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\nimage = Image.open(requests.get(image_url, stream=True).raw)\n\n# Preprocess the image\nvision_input = preprocess_image(image)\n\n# Rest of your code for exporting to ONNX\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:42.030091Z","iopub.execute_input":"2023-08-12T17:51:42.030437Z","iopub.status.idle":"2023-08-12T17:51:42.504366Z","shell.execute_reply.started":"2023-08-12T17:51:42.030403Z","shell.execute_reply":"2023-08-12T17:51:42.503348Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from accelerate import dispatch_model, infer_auto_device_map\nfrom accelerate.utils import get_balanced_memory\n\nmax_memory = get_balanced_memory(\n    model,\n    max_memory=None,\n    no_split_module_classes=[\"GPTNeoXLayer\", \"GPTNeoXMLP\"],\n    dtype='float16',\n    low_zero=False,\n)\n\ndevice_map = infer_auto_device_map(\n    model,\n    max_memory=max_memory,\n    no_split_module_classes=[\"GPTNeoXLayer\", \"GPTNeoXMLP\"],\n    dtype='float16'\n)\n\nmodel = dispatch_model(model, device_map=device_map)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:42.505636Z","iopub.execute_input":"2023-08-12T17:51:42.505983Z","iopub.status.idle":"2023-08-12T17:51:44.776647Z","shell.execute_reply.started":"2023-08-12T17:51:42.505949Z","shell.execute_reply":"2023-08-12T17:51:44.775665Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:44.778334Z","iopub.execute_input":"2023-08-12T17:51:44.778752Z","iopub.status.idle":"2023-08-12T17:51:44.787818Z","shell.execute_reply.started":"2023-08-12T17:51:44.778714Z","shell.execute_reply":"2023-08-12T17:51:44.786788Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Pix2StructForConditionalGeneration(\n  (encoder): Pix2StructVisionModel(\n    (embeddings): Pix2StructVisionEmbeddings(\n      (patch_projection): Linear(in_features=768, out_features=768, bias=True)\n      (row_embedder): Embedding(4096, 768)\n      (column_embedder): Embedding(4096, 768)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): Pix2StructVisionEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x Pix2StructVisionLayer(\n          (attention): Pix2StructVisionAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (mlp): Pix2StructVisionMlp(\n            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=768, bias=False)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (act): NewGELUActivation()\n          )\n          (pre_mlp_layer_norm): Pix2StructLayerNorm()\n          (pre_attention_layer_norm): Pix2StructLayerNorm()\n        )\n      )\n    )\n    (layernorm): Pix2StructLayerNorm()\n  )\n  (decoder): Pix2StructTextModel(\n    (embed_tokens): Embedding(50244, 768)\n    (layer): ModuleList(\n      (0): Pix2StructTextBlock(\n        (self_attention): Pix2StructTextLayerSelfAttention(\n          (attention): Pix2StructTextAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n            (relative_attention_bias): Embedding(32, 12)\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n          (attention): Pix2StructTextAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (mlp): Pix2StructTextLayerFF(\n          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): NewGELUActivation()\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1-11): 11 x Pix2StructTextBlock(\n        (self_attention): Pix2StructTextLayerSelfAttention(\n          (attention): Pix2StructTextAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder_decoder_attention): Pix2StructTextLayerCrossAttention(\n          (attention): Pix2StructTextAttention(\n            (query): Linear(in_features=768, out_features=768, bias=False)\n            (key): Linear(in_features=768, out_features=768, bias=False)\n            (value): Linear(in_features=768, out_features=768, bias=False)\n            (output): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (mlp): Pix2StructTextLayerFF(\n          (DenseReluDense): Pix2StructTextDenseGatedActDense(\n            (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n            (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n            (wo): Linear(in_features=2048, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): NewGELUActivation()\n          )\n          (layer_norm): Pix2StructLayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (final_layer_norm): Pix2StructLayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n    (lm_head): Linear(in_features=768, out_features=50244, bias=False)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in model.named_parameters():\n    print(f\"{i[0]} -> {i[1].device}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:44.789052Z","iopub.execute_input":"2023-08-12T17:51:44.789604Z","iopub.status.idle":"2023-08-12T17:51:44.804267Z","shell.execute_reply.started":"2023-08-12T17:51:44.789565Z","shell.execute_reply":"2023-08-12T17:51:44.803254Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"encoder.embeddings.patch_projection.weight -> cuda:0\nencoder.embeddings.patch_projection.bias -> cuda:0\nencoder.embeddings.row_embedder.weight -> cuda:0\nencoder.embeddings.column_embedder.weight -> cuda:0\nencoder.encoder.layer.0.attention.query.weight -> cuda:0\nencoder.encoder.layer.0.attention.key.weight -> cuda:0\nencoder.encoder.layer.0.attention.value.weight -> cuda:0\nencoder.encoder.layer.0.attention.output.weight -> cuda:0\nencoder.encoder.layer.0.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.0.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.0.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.0.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.0.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.1.attention.query.weight -> cuda:0\nencoder.encoder.layer.1.attention.key.weight -> cuda:0\nencoder.encoder.layer.1.attention.value.weight -> cuda:0\nencoder.encoder.layer.1.attention.output.weight -> cuda:0\nencoder.encoder.layer.1.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.1.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.1.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.1.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.1.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.2.attention.query.weight -> cuda:0\nencoder.encoder.layer.2.attention.key.weight -> cuda:0\nencoder.encoder.layer.2.attention.value.weight -> cuda:0\nencoder.encoder.layer.2.attention.output.weight -> cuda:0\nencoder.encoder.layer.2.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.2.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.2.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.2.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.2.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.3.attention.query.weight -> cuda:0\nencoder.encoder.layer.3.attention.key.weight -> cuda:0\nencoder.encoder.layer.3.attention.value.weight -> cuda:0\nencoder.encoder.layer.3.attention.output.weight -> cuda:0\nencoder.encoder.layer.3.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.3.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.3.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.3.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.3.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.4.attention.query.weight -> cuda:0\nencoder.encoder.layer.4.attention.key.weight -> cuda:0\nencoder.encoder.layer.4.attention.value.weight -> cuda:0\nencoder.encoder.layer.4.attention.output.weight -> cuda:0\nencoder.encoder.layer.4.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.4.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.4.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.4.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.4.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.5.attention.query.weight -> cuda:0\nencoder.encoder.layer.5.attention.key.weight -> cuda:0\nencoder.encoder.layer.5.attention.value.weight -> cuda:0\nencoder.encoder.layer.5.attention.output.weight -> cuda:0\nencoder.encoder.layer.5.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.5.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.5.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.5.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.5.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.6.attention.query.weight -> cuda:0\nencoder.encoder.layer.6.attention.key.weight -> cuda:0\nencoder.encoder.layer.6.attention.value.weight -> cuda:0\nencoder.encoder.layer.6.attention.output.weight -> cuda:0\nencoder.encoder.layer.6.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.6.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.6.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.6.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.6.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.7.attention.query.weight -> cuda:0\nencoder.encoder.layer.7.attention.key.weight -> cuda:0\nencoder.encoder.layer.7.attention.value.weight -> cuda:0\nencoder.encoder.layer.7.attention.output.weight -> cuda:0\nencoder.encoder.layer.7.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.7.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.7.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.7.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.7.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.8.attention.query.weight -> cuda:0\nencoder.encoder.layer.8.attention.key.weight -> cuda:0\nencoder.encoder.layer.8.attention.value.weight -> cuda:0\nencoder.encoder.layer.8.attention.output.weight -> cuda:0\nencoder.encoder.layer.8.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.8.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.8.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.8.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.8.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.9.attention.query.weight -> cuda:0\nencoder.encoder.layer.9.attention.key.weight -> cuda:0\nencoder.encoder.layer.9.attention.value.weight -> cuda:0\nencoder.encoder.layer.9.attention.output.weight -> cuda:0\nencoder.encoder.layer.9.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.9.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.9.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.9.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.9.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.10.attention.query.weight -> cuda:0\nencoder.encoder.layer.10.attention.key.weight -> cuda:0\nencoder.encoder.layer.10.attention.value.weight -> cuda:0\nencoder.encoder.layer.10.attention.output.weight -> cuda:0\nencoder.encoder.layer.10.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.10.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.10.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.10.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.10.pre_attention_layer_norm.weight -> cuda:0\nencoder.encoder.layer.11.attention.query.weight -> cuda:0\nencoder.encoder.layer.11.attention.key.weight -> cuda:0\nencoder.encoder.layer.11.attention.value.weight -> cuda:0\nencoder.encoder.layer.11.attention.output.weight -> cuda:0\nencoder.encoder.layer.11.mlp.wi_0.weight -> cuda:0\nencoder.encoder.layer.11.mlp.wi_1.weight -> cuda:0\nencoder.encoder.layer.11.mlp.wo.weight -> cuda:0\nencoder.encoder.layer.11.pre_mlp_layer_norm.weight -> cuda:0\nencoder.encoder.layer.11.pre_attention_layer_norm.weight -> cuda:0\nencoder.layernorm.weight -> cuda:0\ndecoder.embed_tokens.weight -> cuda:1\ndecoder.layer.0.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.0.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.0.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.0.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.0.self_attention.attention.relative_attention_bias.weight -> cuda:1\ndecoder.layer.0.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.0.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.0.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.0.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.0.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.0.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.0.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.0.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.0.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.0.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.1.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.1.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.1.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.1.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.1.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.1.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.1.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.1.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.1.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.1.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.1.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.1.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.1.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.1.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.2.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.2.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.2.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.2.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.2.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.2.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.2.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.2.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.2.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.2.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.2.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.2.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.2.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.2.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.3.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.3.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.3.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.3.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.3.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.3.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.3.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.3.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.3.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.3.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.3.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.3.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.3.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.3.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.4.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.4.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.4.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.4.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.4.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.4.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.4.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.4.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.4.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.4.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.4.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.4.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.4.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.4.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.5.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.5.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.5.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.5.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.5.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.5.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.5.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.5.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.5.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.5.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.5.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.5.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.5.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.5.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.6.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.6.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.6.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.6.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.6.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.6.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.6.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.6.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.6.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.6.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.6.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.6.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.6.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.6.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.7.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.7.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.7.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.7.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.7.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.7.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.7.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.7.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.7.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.7.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.7.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.7.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.7.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.7.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.8.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.8.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.8.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.8.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.8.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.8.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.8.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.8.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.8.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.8.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.8.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.8.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.8.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.8.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.9.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.9.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.9.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.9.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.9.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.9.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.9.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.9.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.9.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.9.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.9.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.9.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.9.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.9.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.10.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.10.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.10.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.10.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.10.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.10.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.10.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.10.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.10.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.10.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.10.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.10.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.10.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.10.mlp.layer_norm.weight -> cuda:1\ndecoder.layer.11.self_attention.attention.query.weight -> cuda:1\ndecoder.layer.11.self_attention.attention.key.weight -> cuda:1\ndecoder.layer.11.self_attention.attention.value.weight -> cuda:1\ndecoder.layer.11.self_attention.attention.output.weight -> cuda:1\ndecoder.layer.11.self_attention.layer_norm.weight -> cuda:1\ndecoder.layer.11.encoder_decoder_attention.attention.query.weight -> cuda:1\ndecoder.layer.11.encoder_decoder_attention.attention.key.weight -> cuda:1\ndecoder.layer.11.encoder_decoder_attention.attention.value.weight -> cuda:1\ndecoder.layer.11.encoder_decoder_attention.attention.output.weight -> cuda:1\ndecoder.layer.11.encoder_decoder_attention.layer_norm.weight -> cuda:1\ndecoder.layer.11.mlp.DenseReluDense.wi_0.weight -> cuda:1\ndecoder.layer.11.mlp.DenseReluDense.wi_1.weight -> cuda:1\ndecoder.layer.11.mlp.DenseReluDense.wo.weight -> cuda:1\ndecoder.layer.11.mlp.layer_norm.weight -> cuda:1\ndecoder.final_layer_norm.weight -> cuda:1\ndecoder.lm_head.weight -> cuda:1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model checkpoint\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    # Add other necessary information\n}, 'model_checkpoint.pth')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:44.805685Z","iopub.execute_input":"2023-08-12T17:51:44.806299Z","iopub.status.idle":"2023-08-12T17:51:47.076879Z","shell.execute_reply.started":"2023-08-12T17:51:44.806265Z","shell.execute_reply":"2023-08-12T17:51:47.074484Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Save the model checkpoint with your desired filename\ncheckpoint_filename = 'my_model_checkpoint.pth'\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    # Add other necessary information\n}, checkpoint_filename)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:47.082841Z","iopub.execute_input":"2023-08-12T17:51:47.088373Z","iopub.status.idle":"2023-08-12T17:51:50.232590Z","shell.execute_reply.started":"2023-08-12T17:51:47.088303Z","shell.execute_reply":"2023-08-12T17:51:50.231503Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING=1 \n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:50.234242Z","iopub.execute_input":"2023-08-12T17:51:50.234660Z","iopub.status.idle":"2023-08-12T17:51:50.239919Z","shell.execute_reply.started":"2023-08-12T17:51:50.234624Z","shell.execute_reply":"2023-08-12T17:51:50.239019Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING=1 \n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:50.241513Z","iopub.execute_input":"2023-08-12T17:51:50.242248Z","iopub.status.idle":"2023-08-12T17:51:51.576116Z","shell.execute_reply.started":"2023-08-12T17:51:50.242213Z","shell.execute_reply":"2023-08-12T17:51:51.574957Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#convert.py\nimport torch\nimport torch.onnx\n\n# Load your model and set device (replace this with your actual model loading code)\n\ndevice = \"cuda:0\"  # or \"cpu\" if you prefer\nmodel.to(device)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Example input image (adjust as needed)\ninput_image = torch.randn(3, 768, 768).to(device)\n\n# Define your preprocess_function to match the model's input format\ndef preprocess_function(input_tensor):\n    # Apply necessary preprocessing transformations\n    return input_tensor\n\n# Preprocess the input image if needed\n# Here you can apply any necessary transformations to match the model's input format\npreprocessed_input = preprocess_function(input_image)\n\n# Forward pass\nwith torch.no_grad():\n    output = model(preprocessed_input)\n    \n# Define your postprocess_function to match the expected output format\ndef postprocess_function(output_tensor):\n    # Apply necessary postprocessing transformations\n    return output_tensor\n\n# Postprocess the output to match the expected output format\n# Here you can apply any necessary transformations to the model's output\npostprocessed_output = postprocess_function(output)\n\n# Export the model to ONNX\noutput_path = \"/kaggle/working/my_model_checkpoint.pth\"\ninput_names = [\"input\"]\noutput_names = [\"output\"]\n\n# Exporting the model to ONNX\ntorch.onnx.export(\n    model,\n    preprocessed_input,\n    output_path,\n    input_names=input_names,\n    output_names=output_names,\n    verbose=True,\n    opset_version=12  # Use the appropriate ONNX opset version\n)\n\nprint(f\"Model exported to {output_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T18:00:00.578412Z","iopub.execute_input":"2023-08-12T18:00:00.579510Z","iopub.status.idle":"2023-08-12T18:00:01.885148Z","shell.execute_reply.started":"2023-08-12T18:00:00.579470Z","shell.execute_reply":"2023-08-12T18:00:01.883487Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m27\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m# Forward pass\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m27 \u001b[2m│   \u001b[0moutput = model(preprocessed_input)                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m# Define your postprocess_function to match the expected output format\u001b[0m                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mpostprocess_function\u001b[0m(output_tensor):                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/pix2struct/\u001b[0m\u001b[1;33mmodeling_pix2struct.py\u001b[0m:\u001b[94m17\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m29\u001b[0m in \u001b[92mforward\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1726 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1727 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Encode if needed (training, first prediction pass)\u001b[0m                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1728 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m encoder_outputs \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1729 \u001b[2m│   │   │   \u001b[0mencoder_outputs = \u001b[96mself\u001b[0m.encoder(                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1730 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mflattened_patches=flattened_patches,                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1731 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mattention_mask=attention_mask,                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1732 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhead_mask=head_mask,                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/pix2struct/\u001b[0m\u001b[1;33mmodeling_pix2struct.py\u001b[0m:\u001b[94m63\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m7\u001b[0m in \u001b[92mforward\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 634 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x s\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 635 \u001b[0m\u001b[2m│   │   \u001b[0mhead_mask = \u001b[96mself\u001b[0m.get_head_mask(head_mask, \u001b[96mself\u001b[0m.config.num_hidden_layers)          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 636 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 637 \u001b[2m│   │   \u001b[0membedding_output = \u001b[96mself\u001b[0m.embeddings(flattened_patches)                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 638 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 639 \u001b[0m\u001b[2m│   │   \u001b[0mencoder_outputs = \u001b[96mself\u001b[0m.encoder(                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 640 \u001b[0m\u001b[2m│   │   │   \u001b[0membedding_output,                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/pix2struct/\u001b[0m\u001b[1;33mmodeling_pix2struct.py\u001b[0m:\u001b[94m14\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m1\u001b[0m in \u001b[92mforward\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 138 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 139 \u001b[0m\u001b[2m│   │   \u001b[0mflattened_patches = flattened_patches[:, :, \u001b[94m2\u001b[0m:]                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 140 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 141 \u001b[2m│   │   \u001b[0membeddings = \u001b[96mself\u001b[0m.patch_projection(flattened_patches)                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 142 \u001b[0m\u001b[2m│   │   \u001b[0mrow_embeddings = \u001b[96mself\u001b[0m.row_embedder(row_indices)                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 143 \u001b[0m\u001b[2m│   │   \u001b[0mcol_embeddings = \u001b[96mself\u001b[0m.column_embedder(col_indices)                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 144 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mlinear.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92mforward\u001b[0m                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   \u001b[0minit.uniform_(\u001b[96mself\u001b[0m.bias, -bound, bound)                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor) -> Tensor:                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m F.linear(\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.bias)                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mextra_repr\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mstr\u001b[0m:                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[33m'\u001b[0m\u001b[33min_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, out_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, bias=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m.format(                          \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mRuntimeError: \u001b[0mmat1 and mat2 shapes cannot be multiplied \u001b[1m(\u001b[0m2304x766 and 768x768\u001b[1m)\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">27</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 # Forward pass</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>27 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>output = model(preprocessed_input)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 # Define your postprocess_function to match the expected output format</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">postprocess_function</span>(output_tensor):                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/pix2struct/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_pix2struct.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">29</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1726 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1727 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Encode if needed (training, first prediction pass)</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1728 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> encoder_outputs <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1729 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>encoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder(                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1730 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>flattened_patches=flattened_patches,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1731 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>attention_mask=attention_mask,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1732 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>head_mask=head_mask,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/pix2struct/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_pix2struct.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">63</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 634 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># and head_mask is converted to shape [num_hidden_layers x batch x num_heads x s</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 635 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>head_mask = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_head_mask(head_mask, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.num_hidden_layers)          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 636 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 637 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>embedding_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embeddings(flattened_patches)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 638 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 639 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>encoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 640 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>embedding_output,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/pix2struct/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_pix2struct.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 138 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 139 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>flattened_patches = flattened_patches[:, :, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:]                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 140 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 141 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>embeddings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.patch_projection(flattened_patches)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 142 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>row_embeddings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.row_embedder(row_indices)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 143 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>col_embeddings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.column_embedder(col_indices)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 144 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>init.uniform_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, -bound, bound)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor) -&gt; Tensor:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">extra_repr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #808000; text-decoration-color: #808000\">'in_features={}, out_features={}, bias={}'</span>.format(                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>mat1 and mat2 shapes cannot be multiplied <span style=\"font-weight: bold\">(</span>2304x766 and 768x768<span style=\"font-weight: bold\">)</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.onnx\nimport onnxruntime\nimport requests\nfrom PIL import Image\nimport time\nimport os\n\n# Load the original Hugging Face model (replace with your model loading code)\n# model = ...  # Load your Hugging Face model here\n\n# Export the Hugging Face model to ONNX format\nonnx_model_path = \"/kaggle/working/my_model_checkpoint.pth\"\ndummy_input = torch.randn(3, 768, 768)\ntorch.onnx.export(model, dummy_input, onnx_model_path, verbose=True, opset_version=12)\n\n# Load the ONNX model\nonnx_session = onnxruntime.InferenceSession(onnx_model_path)\n\n# Load example image\nimage_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets_logo_name.png\"\nimage = Image.open(requests.get(image_url, stream=True).raw)\n\n# Preprocess the input image\ninput_image = image.resize((768, 768))\ninput_image = torch.tensor([torchvision.transforms.functional.to_tensor(input_image)])\n\n# Measure size of the models\nhugging_face_model_size_mb = os.path.getsize(\"/kaggle/working/model_checkpoint.pth\") / (1024 * 1024)\nonnx_model_size_mb = os.path.getsize(onnx_model_path) / (1024 * 1024)\n\n# Measure inference time for Hugging Face model\nstart_time = time.time()\nwith torch.no_grad():\n    _ = model(input_image)\nhugging_face_inference_time = time.time() - start_time\n\n# Measure inference time for ONNX model\nstart_time = time.time()\nonnx_input = {\"input\": input_image.numpy()}\n_ = onnx_session.run(None, onnx_input)\nonnx_inference_time = time.time() - start_time\n\n# Display results\nprint(\"Size of Hugging Face model:\", hugging_face_model_size_mb, \"MB\")\nprint(\"Size of ONNX model:\", onnx_model_size_mb, \"MB\")\nprint(\"Inference time of Hugging Face model:\", hugging_face_inference_time, \"seconds\")\nprint(\"Inference time of ONNX model:\", onnx_inference_time, \"seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-12T17:51:55.585708Z","iopub.status.idle":"2023-08-12T17:51:55.590707Z","shell.execute_reply.started":"2023-08-12T17:51:55.590458Z","shell.execute_reply":"2023-08-12T17:51:55.590479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}